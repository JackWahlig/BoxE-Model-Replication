{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Fiz3JO227Ae",
    "outputId": "03c93d16-73b6-4b07-9164-35fa4409cdcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGWCBcRXWlPq"
   },
   "outputs": [],
   "source": [
    "class BoxELoss():\n",
    "  def __init__(self, options):\n",
    "    if options.loss_type in ['uniform', 'u']:\n",
    "      self.loss_fn = uniform_loss\n",
    "      self.fn_kwargs = {'gamma': options.margin, 'w': 1/options.loss_k}\n",
    "    elif options.loss_type in ['adversarial', 'self-adversarial', 'self adversarial', 'a']:\n",
    "      self.loss_fn = adversarial_loss\n",
    "      self.fn_kwargs = {'gamma': options.margin, 'alpha': options.adversarial_temp}\n",
    "\n",
    "  def __call__(self, entity_final_emb, box_low, box_high, neg_emb, neg_box_low, neg_box_high):\n",
    "    return self.loss_fn(entity_final_emb, box_low, box_high, neg_emb, neg_box_low, neg_box_high, **self.fn_kwargs)\n",
    "\n",
    "\n",
    "def dist(entity_emb, boxes):\n",
    "  lb = boxes[0,:,:]  # lower boundries\n",
    "  ub = boxes[1,:,:]  # upper boundries\n",
    "  c = (lb + ub)/2  # centres\n",
    "  w = ub - lb + 1  # widths\n",
    "  k = 0.5*(w - 1) * (w - 1/w)\n",
    "  return torch.where(torch.logical_and(torch.ge(entity_emb, lb), torch.le(entity_emb, ub)),\n",
    "                     torch.abs(entity_emb - c) / w,\n",
    "                     torch.abs(entity_emb - c) * w - k)\n",
    "\n",
    "def score(r_headbox, r_tailbox, e_head, e_tail, order=2):\n",
    "  # once the representaion of r is known this should probably just take r and the entities\n",
    "  return torch.norm(dist(e_head, r_headbox), p=order) + torch.norm(dist(e_tail, r_tailbox), p=order, dim=1)\n",
    "\n",
    "'''\n",
    "Same as the score function, but adapted to the embedding representation created by the model forward pass\n",
    "'''\n",
    "def score_(point_embs, box_low, box_high, order=2):\n",
    "  e_head, e_tail = point_embs[:,0,:], point_embs[:,1,:]\n",
    "  headboxes, tailboxes = torch.stack((box_low[:,0,:], box_high[:,0,:]), dim=0), torch.stack((box_low[:,1,:], box_high[:,1,:]), dim=0)\n",
    "  return score(headboxes, tailboxes, e_head, e_tail, order)\n",
    "\n",
    "\n",
    "def uniform_loss(point_embs, box_low, box_high, negative_point_embs, negative_box_low, negative_box_high, gamma, w):\n",
    "  #TODO verify results of this function\n",
    "  e_head, e_tail = point_embs[:,0,:], point_embs[:,1,:]\n",
    "  headboxes, tailboxes = torch.stack((box_low[:,0,:], box_high[:,0,:]), dim=0), torch.stack((box_low[:,1,:], box_high[:,1,:]), dim=0)\n",
    "  s1 = - torch.log(torch.sigmoid(gamma - score(headboxes, tailboxes, e_head, e_tail)))\n",
    "  s2_terms = []\n",
    "  if not torch.is_tensor(w):\n",
    "    w = torch.tensor([w]).repeat(len(negative_point_embs))\n",
    "  for i in range(len(negative_point_embs)):\n",
    "    ne_head, ne_tail = negative_point_embs[i,:,0,:], negative_point_embs[i,:,1,:]\n",
    "    nheadboxes, ntailboxes = torch.stack((negative_box_low[i,:,0,:], negative_box_high[i,:,0,:]), dim=0), torch.stack((negative_box_low[i,:,1,:], negative_box_high[i,:,1,:]), dim=0)\n",
    "    s2_terms.append(w[i] * torch.log(torch.sigmoid(score(nheadboxes, ntailboxes, ne_head, ne_tail) - gamma)))\n",
    "  s2 = torch.sum(torch.stack(s2_terms), dim=0)\n",
    "  return torch.sum(s1 - s2)\n",
    "\n",
    "def triple_probs(point_embs, box_low, box_high, alpha):\n",
    "  scores = []\n",
    "  for i in range(len(point_embs)):\n",
    "    e_head, e_tail = point_embs[i,:,0,:], point_embs[i,:,1,:]\n",
    "    headboxes, tailboxes = torch.stack((box_low[i,:,0,:], box_high[i,:,0,:]), dim=0), torch.stack((box_low[i,:,1,:], box_high[i,:,1,:]), dim=0)\n",
    "    scores.append(torch.exp(alpha * score(headboxes, tailboxes, e_head, e_tail)))\n",
    "  scores = torch.stack(scores)\n",
    "  div = torch.repeat_interleave(torch.sum(scores, dim=1).unsqueeze(1), repeats=scores.shape[1], dim=1)\n",
    "  return torch.div(scores, div)\n",
    "  \n",
    "\n",
    "def adversarial_loss(point_embs, box_low, box_high, negative_point_embs, negative_box_low, negative_box_high, gamma, alpha):\n",
    "  triple_weights = triple_probs(negative_point_embs, negative_box_low, negative_box_high, alpha)\n",
    "  return uniform_loss(point_embs, box_low, box_high, negative_point_embs, negative_box_low, negative_box_high, gamma, triple_weights)\n",
    "\n",
    "def get_loss_function(loss_type='uniform'):\n",
    "  if loss_type in ['uniform', 'u']:\n",
    "    return uniform_loss\n",
    "  elif loss_type in ['adversarial', 'self-adversarial', 'self adversarial', 'a']:\n",
    "    return adversarial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJOTDpF1btfY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', default='./data/',\n",
    "                        help='Path to datasets')\n",
    "parser.add_argument('--data_name', default='FB15k',\n",
    "                        help='Name of knowledge graph')\n",
    "parser.add_argument('--margin', default=0.2, type=float,\n",
    "                        help='Loss margin.')\n",
    "parser.add_argument('--num_epochs', default=10, type=int,\n",
    "                        help='Number of training epochs.')\n",
    "parser.add_argument('--batch_size', default=128, type=int,\n",
    "                        help='Size of a training batch size.')\n",
    "parser.add_argument('--embedding_dim', default=300, type=int,\n",
    "                        help='Dimensionality of the embedding.')\n",
    "parser.add_argument('--grad_clip', default=2., type=float,\n",
    "                        help='Gradient clipping threshold.')\n",
    "parser.add_argument('--learning_rate', default=.0001, type=float,\n",
    "                        help='Learning rate.')\n",
    "parser.add_argument('--log_step', default=10, type=int,\n",
    "                        help='Number of steps to print and record the log.')\n",
    "parser.add_argument('--val_step', default=500, type=int,\n",
    "                        help='Number of steps to run validation.')\n",
    "parser.add_argument('--normed_bumps', action='store_true',\n",
    "                        help='Do not normalize the image embeddings.')\n",
    "parser.add_argument('--truncate_datasets', default=-1, type=int,\n",
    "                        help='Truncate datasets to a subset of entries')\n",
    "parser.add_argument('--adversarial_temp', default=1, type=float,\n",
    "                        help='Alpha parameter for adversarial negative sampling loss')\n",
    "parser.add_argument('--loss_k', default=1, type=float,\n",
    "                        help='k parameter for uniform loss')\n",
    "parser.add_argument('--loss_type', default='u', type=str,\n",
    "                        help=\"Toggle between uniform ('u') and self-adversarial ('a') loss\")\n",
    "parser.add_argument('--num_negative_samples', default=10, type=int,\n",
    "                        help=\"Number of negative samples per positive (true) triple\")\n",
    "opt = parser.parse_args(['--data_path', '', '--data_name', '', '--batch_size', '50', '--embedding_dim', '5',\n",
    "                         '--learning_rate', '0.01', '--num_epochs', '50', '--truncate', '200', '--loss_type', 'a'])\n",
    "print(opt)\n",
    "kg = Data(opt, truncate=opt.truncate_datasets)\n",
    "train_loader, val_loader, test_loader = kg.get_loaders()\n",
    "opt.num_entity, opt.num_relation = kg.num_entity, kg.num_relation\n",
    "\n",
    "model = BoxEModel(opt)\n",
    "optimizer = Adam(model.parameters(), lr=opt.learning_rate)\n",
    "loss_fn = BoxELoss(opt)\n",
    "\n",
    "for epoch in range(opt.num_epochs):\n",
    "  for i, data in enumerate(train_loader):\n",
    "    # switch to train mode\n",
    "    #model.train()\n",
    "    negatives = kg.sample_negatives(data, 4)\n",
    "    optimizer.zero_grad()\n",
    "    entity_final_emb, box_low, box_high = model.forward(data)\n",
    "    neg_emb, neg_box_low, neg_box_high = model.forward_negatives(negatives)\n",
    "    loss = loss_fn(entity_final_emb, box_low, box_high, neg_emb, neg_box_low, neg_box_high)\n",
    "    if i % 100 == 0:\n",
    "      print('LOSS: {}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzK4ADy3JDXB"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# train.py\n",
    "#----------------------\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "def main(): # Jiaqi\n",
    "    settings = ['--data_path', '', '--data_name', '', '--batch_size', '1024',\n",
    "                '--embedding_dim', '100', '--learning_rate', '0.001', '--num_epochs',\n",
    "                '500', '--loss_type', 'u', '--num_negative_samples', '100',\n",
    "                '--margin', '18', '--val_step', '100']\n",
    "    # Hyper Parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', default='./data/',\n",
    "                        help='Path to datasets')\n",
    "    parser.add_argument('--data_name', default='FB15k',\n",
    "                        help='Name of knowledge graph')\n",
    "    parser.add_argument('--margin', default=0.2, type=float,\n",
    "                        help='Loss margin.')\n",
    "    parser.add_argument('--num_epochs', default=10, type=int,\n",
    "                        help='Number of training epochs.')\n",
    "    parser.add_argument('--batch_size', default=128, type=int,\n",
    "                        help='Size of a training batch size.')\n",
    "    parser.add_argument('--embedding_dim', default=300, type=int,\n",
    "                        help='Dimensionality of the embedding.')\n",
    "    parser.add_argument('--grad_clip', default=2., type=float,\n",
    "                        help='Gradient clipping threshold.')\n",
    "    parser.add_argument('--learning_rate', default=.0001, type=float,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--log_step', default=10, type=int,\n",
    "                        help='Number of steps to print and record the log.')\n",
    "    parser.add_argument('--val_step', default=500, type=int,\n",
    "                        help='Number of steps to run validation.')\n",
    "    parser.add_argument('--normed_bumps', action='store_true',\n",
    "                        help='Do not normalize the image embeddings.')\n",
    "    parser.add_argument('--truncate_datasets', default=-1, type=int,\n",
    "                        help='Truncate datasets to a subset of entries')\n",
    "    parser.add_argument('--adversarial_temp', default=1, type=float,\n",
    "                        help='Alpha parameter for adversarial negative sampling loss')\n",
    "    parser.add_argument('--loss_k', default=1, type=float,\n",
    "                        help='k parameter for uniform loss')\n",
    "    parser.add_argument('--loss_type', default='u', type=str,\n",
    "                        help=\"Toggle between uniform ('u') and self-adversarial ('a') loss\")\n",
    "    parser.add_argument('--num_negative_samples', default=10, type=int,\n",
    "                        help=\"Number of negative samples per positive (true) triple\")\n",
    "    opt = parser.parse_args(settings)\n",
    "\n",
    "    print(opt)\n",
    "\n",
    "    kg = Data(opt, truncate=opt.truncate_datasets)\n",
    "    # Load data loaders\n",
    "    train_loader, val_loader, test_loader = kg.get_loaders()\n",
    "\n",
    "    opt.num_entity, opt.num_relation = kg.num_entity, kg.num_relation\n",
    "\n",
    "    # Construct the model\n",
    "    model = BoxEModel(opt).to(device)\n",
    "    optim = Adam(model.parameters(), lr=opt.learning_rate)\n",
    "    loss_fn = BoxELoss(opt)\n",
    "\n",
    "    # Train the Model\n",
    "    best_val_mrr, best_params = train(opt, model, train_loader, val_loader, optim, kg, loss_fn)\n",
    "\n",
    "    # Test on test-data\n",
    "    model.load_parameters(best_params)\n",
    "    test_results = test(test_loader, opt, model, kg, loss_fn)\n",
    "    return best_val_mrr, best_params, test_results\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, opt, model, knowledge_graph, loss_fn):\n",
    "  print('Testing started')\n",
    "  MR = []\n",
    "  MRR = []\n",
    "  hits1 = []\n",
    "  hits3 = []\n",
    "  accuracy = []\n",
    "  outputs = {}\n",
    "  for i, data in enumerate(dataloader):\n",
    "      data = data.to(device)\n",
    "      output = evaluate(opt, model, data, knowledge_graph, loss_fn)\n",
    "      # val_loss.append(output['loss'])\n",
    "      MR.append(output['MR'])\n",
    "      MRR.append(output['MRR'])\n",
    "      hits1.append(output['hits'][0])\n",
    "      hits3.append(output['hits'][1])\n",
    "      accuracy.append(output['accuracy'])\n",
    "  # val_outputs['loss'] = np.mean(val_loss)\n",
    "  outputs['MR'] = np.mean(MR)\n",
    "  outputs['MRR'] = np.mean(MRR)\n",
    "  outputs['hits1'] = np.mean(hits1)\n",
    "  outputs['hits3'] = np.mean(hits3)\n",
    "  outputs['accuracy'] = np.mean(accuracy)\n",
    "  return outputs\n",
    "\n",
    "\n",
    "'''\n",
    "@brief Train the BoxE model with validation, print result\n",
    "@param BoxE model, train_loader(torch.utils.data.DataLoader), val_loader(torch.utils.data.DataLoader)\n",
    "@return None\n",
    "'''\n",
    "def train(opt, model, train_loader, val_loader, optimizer, knowledge_graph, loss_fn): # Jiaqi\n",
    "    #optimizer = Adam(model.parameters(), lr=opt.learning_rate)\n",
    "    print('Training started')\n",
    "    best_mrr = -1\n",
    "    best_model_params = None\n",
    "\n",
    "    for epoch in range(opt.num_epochs):\n",
    "        loss_list = []\n",
    "        for i, train_data in enumerate(train_loader):\n",
    "            # switch to train mode\n",
    "            train_data = train_data.to(device)\n",
    "            negatives = knowledge_graph.sample_negatives(train_data, opt.num_negative_samples)\n",
    "            optimizer.zero_grad()\n",
    "            entity_final_emb, box_low, box_high = model(train_data)\n",
    "            neg_emb, neg_box_low, neg_box_high = model.forward_negatives(negatives)\n",
    "            loss = loss_fn(entity_final_emb, box_low, box_high, neg_emb, neg_box_low, neg_box_high)\n",
    "            loss_list.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # print mean loss of each epoch\n",
    "        print('Epoch done')\n",
    "        #print(\"epoch: {}; Loss: {}\".format(epoch, np.mean(loss_list)))\n",
    "        # validate on train and validation dataset\n",
    "        if epoch % opt.val_step == 0:\n",
    "            print('Validation checkpoint reached')\n",
    "            # train data  #  this can go away\n",
    "            # tr_loss = []\n",
    "            # tr_MR = []\n",
    "            # tr_MRR = []\n",
    "            # tr_hits1 = []\n",
    "            # tr_hits3 = []\n",
    "            # tr_accuracy = []\n",
    "            # train_outputs = {}\n",
    "            # for i, train_data in enumerate(train_loader):\n",
    "            #     train_data = train_data.to(device)\n",
    "                # output = evaluate(opt, model, train_data, knowledge_graph, loss_fn)\n",
    "                # tr_MR.append(output['MR'])\n",
    "                # tr_MRR.append(output['MRR'])\n",
    "                # tr_hits1.append(output['hits'][0])\n",
    "                # tr_hits3.append(output['hits'][1])\n",
    "                # tr_accuracy.append(output['accuracy'])\n",
    "                # tr_loss.append(output['loss'])\n",
    "            # train_outputs['loss'] = np.mean(tr_loss)\n",
    "            # train_outputs['MR'] = np.mean(tr_MR)\n",
    "            # train_outputs['MRR'] = np.mean(tr_MRR)\n",
    "            # #train_outputs['hits'][0] = np.mean(tr_hits1)\n",
    "            # #train_outputs['hits'][1] = np.mean(tr_hits1)\n",
    "            # train_outputs['hits1'] = np.mean(tr_hits1)\n",
    "            # train_outputs['hits3'] = np.mean(tr_hits3)\n",
    "            # train_outputs['accuracy'] = np.mean(tr_accuracy)\n",
    "            # print_result(opt, 'train', epoch, train_outputs)\n",
    "\n",
    "            # valid data\n",
    "            # val_loss = []\n",
    "            val_MR = []\n",
    "            val_MRR = []\n",
    "            val_hits1 = []\n",
    "            val_hits3 = []\n",
    "            val_accuracy = []\n",
    "            val_outputs = {}\n",
    "            for i, val_data in enumerate(val_loader):\n",
    "                val_data = val_data.to(device)\n",
    "                output = evaluate(opt, model, val_data, knowledge_graph, loss_fn)\n",
    "                # val_loss.append(output['loss'])\n",
    "                val_MR.append(output['MR'])\n",
    "                val_MRR.append(output['MRR'])\n",
    "                val_hits1.append(output['hits'][0])\n",
    "                val_hits3.append(output['hits'][1])\n",
    "                val_accuracy.append(output['accuracy'])\n",
    "            # val_outputs['loss'] = np.mean(val_loss)\n",
    "            val_outputs['MR'] = np.mean(val_MR)\n",
    "            val_outputs['MRR'] = np.mean(val_MRR)\n",
    "            val_outputs['hits1'] = np.mean(val_hits1)\n",
    "            val_outputs['hits3'] = np.mean(val_hits3)\n",
    "            val_outputs['accuracy'] = np.mean(val_accuracy)\n",
    "            print_result(opt, 'val', epoch, val_outputs)\n",
    "            if val_outputs['MRR'] > best_mrr:\n",
    "              best_mrr = val_outputs['MRR']\n",
    "              best_model_params = model.parameter_dict()\n",
    "    print('Training complete')\n",
    "    return best_mrr, best_model_params\n",
    "\n",
    "\n",
    "def print_result(opt, string, epoch, output):\n",
    "    print(\"BoxEModel \" + string + \"results:\")\n",
    "    print(\"Number of Epochs: \" + str(epoch))\n",
    "    print(\"Training for \" + str(opt.data_name))\n",
    "    print(\"Learning Rate: \" + str(opt.learning_rate))\n",
    "    print(\"Embedding Dimension: \" + str(opt.embedding_dim))\n",
    "    # print(\"Loss: \" + str(output['loss']))\n",
    "    print(\"MR: \" + str(output['MR']))\n",
    "    print(\"MRR: \" + str(output['MRR']))\n",
    "    print(\"h@1: \" + str(output['hits1']))\n",
    "    print(\"h@3: \" + str(output['hits3']))\n",
    "    # print(\"h@5: \" + output['hits'][2])\n",
    "    # print(\"h@10: \" + output['hits'][3])\n",
    "    print(\"Accuracy: \" + str(output['accuracy']))\n",
    "\n",
    "\n",
    "'''\n",
    "@brief Evaluate the training, validation and test dataset\n",
    "@param BoxE model, data (train/val/test: [number of facts, 3] (sec_dim: relation, entity1, entity2))\n",
    "@return outputs: evaluation results (type: a dictionary containing loss, MR, MRR, hits, accuracy)\n",
    "'''\n",
    "def evaluate(opt, model, data, knowledge_graph, loss_fn): # Jiaqi\n",
    "    #model.eval()\n",
    "    # negative_samples = knowledge_graph.sample_negatives(data, opt.num_negative_samples)\n",
    "    # with torch.no_grad():\n",
    "    #     entity_final_emb, box_low, box_high = model(data)\n",
    "        # neg_emb, neg_box_low, neg_box_high = model.forward_negatives(negative_samples)\n",
    "\n",
    "    # output evaluation results\n",
    "    outputs = {}\n",
    "\n",
    "    # evaluate input data\n",
    "    # loss =  loss_fn(entity_final_emb, box_low, box_high, neg_emb, neg_box_low, neg_box_high)  # this can go away\n",
    "    # scores = score_(entity_final_emb, box_low, box_high)\n",
    "\n",
    "    # data mask of entity pairs in data\n",
    "    # data_mask = torch.zeros([data.shape[0], opt.num_entity, opt.num_entity])\n",
    "    # data mask of all entity pairs that need to be calculated for evaluate data\n",
    "    # data_eval_mask = torch.zeros([data.shape[0], opt.num_entity, opt.num_entity])\n",
    "\n",
    "    # store the scores of all r(_,t) and r(h,_)\n",
    "    data_eval_score = torch.zeros([data.shape[0], opt.num_entity, opt.num_entity])\n",
    "    # data need to be fed into model for evaluation (all data of r(_,t) and r(h,_))\n",
    "    eval_data = []\n",
    "\n",
    "    count = 0\n",
    "    for item in data: # data: two-dimension->[batch_size, 3]  item: one-dimension->(rel, ent1, ent2)  eval_data: two-dimension->[data.shape[0]*opt.num_entity*2, 3]\n",
    "        # data_mask[count, item[1], item[2]] = 1\n",
    "        # data_eval_mask[count, item[1], :] = 1\n",
    "        # data_eval_mask[count, :, item[2]] = 1\n",
    "        for ent in range(opt.num_entity):\n",
    "            eval_data.append([item[0], item[1], ent])\n",
    "        for ent in range(opt.num_entity):\n",
    "            eval_data.append([item[0], ent, item[2]])\n",
    "        count += 1\n",
    "    eval_data = torch.Tensor(eval_data)\n",
    "    eval_data = eval_data.int().to(device)\n",
    "    # eval_negatives = knowledge_graph.sample_negatives(eval_data, opt.num_negative_samples)\n",
    "    eval_entity_final_emb, eval_box_low, eval_box_high = model(eval_data)\n",
    "    # eval_neg_emb, eval_neg_box_low, eval_neg_box_high = model.forward_negatives(eval_negatives)  # this can go away \n",
    "    # eval_loss = loss_fn(eval_entity_final_emb, eval_box_low, eval_box_high, eval_neg_emb, eval_neg_box_low, eval_neg_box_high)  # this can go away\n",
    "    eval_scores = score_(eval_entity_final_emb, eval_box_low, eval_box_high)\n",
    "\n",
    "    for item in range(data.shape[0]): # item: index of triple in this batch\n",
    "        for ent in range(opt.num_entity):\n",
    "              data_eval_score[item, data[item, 1], ent] = eval_scores[item*opt.num_entity*2 + ent]\n",
    "        for ent in range(opt.num_entity):\n",
    "              data_eval_score[item, ent, data[item, 2]] = eval_scores[item*opt.num_entity*2 + opt.num_entity + ent]\n",
    "\n",
    "    mr, mrr, hits, acc = metrics(opt, data, data_eval_score, knowledge_graph)\n",
    "    '''\n",
    "    data: [number of facts, 3] (sec_dim: relation, entity1, entity2)\n",
    "    data_eval_score: [number of facts, opt.num_entity, opt.num_entity]\n",
    "             an adjacency matrix: (fisrt-dim: number of facts, sec_dim: number of entities, third_dim: number of entities)\n",
    "             If the nth fact is r(h, t), there will be scores in data_eval_score[n,h,:] and data_eval_score[n,:,t]\n",
    "    '''\n",
    "\n",
    "    # outputs['loss'] = loss\n",
    "    outputs['MR'] = mr\n",
    "    outputs['MRR'] = mrr\n",
    "    outputs['hits'] = hits\n",
    "    outputs['accuracy'] = acc\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "'''\n",
    "@brief Calculate the metrics for input data (MR, MRR. Accuracy, Hits)\n",
    "@param opt parameters, data (train/val/test: [number of facts, 3] (sec_dim: relation, entity1, entity2))\n",
    "@return calculated metrics\n",
    "    [MR: Mean rank] \n",
    "    [MRR: Mean reciprocal rank] \n",
    "    [hits: a list containing Hits@1 and Hits@3] \n",
    "    [ret_acc: accuracy of prediction (only consider our prediction to be correct when entity1 and entity2 both being predicted correctly)]\n",
    "'''\n",
    "def metrics(opt, data, eval_score, knowledge_graph): # Jiaqi\n",
    "    num_fact = data.shape[0] # number of input facts\n",
    "    num_entity = eval_score.shape[1]\n",
    "    acc = []\n",
    "    hit1 = []\n",
    "    hit3 = []\n",
    "    rank = []\n",
    "    for i in range(num_fact):\n",
    "        ent1 = data[i, 1]\n",
    "        ent2 = data[i, 2]\n",
    "        # iterate over eval_score[i, ent1, :] and eval_score[i, :, ent2], filter out positive facts except (i, ent1, ent2)\n",
    "        for j in range(num_entity):\n",
    "            if (data[i, 0], ent1, j) in knowledge_graph.facts_set and j != ent2:\n",
    "                eval_score[i, ent1, j] = float('inf')\n",
    "            if (data[i, 0], j, ent2) in knowledge_graph.facts_set and j != ent1:\n",
    "                eval_score[i, j, ent2] = float('inf')\n",
    "        # sort the rank of r(h,_) and r(_,t) (from high to low)\n",
    "        row_rank = eval_score[i, ent1, :].topk(opt.num_entity, dim=0)[1]\n",
    "        col_rank = eval_score[i, :, ent2].topk(opt.num_entity, dim=0)[1]\n",
    "        # reverse the rank order\n",
    "        row_rank = torch.flip(row_rank.reshape(row_rank.shape[0],1), [0,1])\n",
    "        row_rank = row_rank.reshape(-1)\n",
    "        col_rank = torch.flip(col_rank.reshape(col_rank.shape[0],1), [0,1])\n",
    "        col_rank = col_rank.reshape(-1)\n",
    "        # calculate the metrics\n",
    "        rank.append((row_rank == ent2).nonzero().item() + 1)\n",
    "        rank.append((col_rank == ent1).nonzero().item() + 1)\n",
    "        hit1.append(1 if ((row_rank == ent2).nonzero().item() < 1) else 0)\n",
    "        hit1.append(1 if ((col_rank == ent1).nonzero().item() < 1) else 0)\n",
    "        hit3.append(1 if ((row_rank == ent2).nonzero().item() < 3) else 0)\n",
    "        hit3.append(1 if ((col_rank == ent1).nonzero().item() < 3) else 0)\n",
    "        acc.append(1 if (row_rank[0] == ent2 and col_rank[0] == ent1) else 0)\n",
    "    MR = np.mean(np.array(rank))\n",
    "    MRR = np.mean(1 / np.array(rank))\n",
    "    ret_acc = np.mean(np.array(acc))\n",
    "    ret_hit1 = np.mean(np.array(hit1))\n",
    "    ret_hit3 = np.mean(np.array(hit3))\n",
    "    hits = [ret_hit1, ret_hit3]\n",
    "    return MR, MRR, hits, ret_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLiCZGT2JtQv"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# model.py\n",
    "#----------------------\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "class BoxEModel: #Jiaqi\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt\n",
    "        self.embedding_dim = opt.embedding_dim\n",
    "        self.num_entity = opt.num_entity\n",
    "        self.num_relation = opt.num_relation\n",
    "        self.data_name = opt.data_name\n",
    "        self.sqrt_dim = torch.sqrt(torch.tensor(self.embedding_dim + 0.0))\n",
    "        self.arity = 2\n",
    "\n",
    "        self.entity_shape = [self.num_entity, self.embedding_dim]\n",
    "        # initialize entity embedding\n",
    "        self.entity_points = nn.Embedding(self.num_entity, self.embedding_dim)\n",
    "\n",
    "        # initialize bump embedding\n",
    "        self.entity_bumps = nn.Embedding(self.num_entity, self.embedding_dim)\n",
    "\n",
    "        # Relation Embedding Instantiation\n",
    "        rel_tbl_shape = [self.num_relation, self.arity, self.embedding_dim]\n",
    "        scale_multiples_shape = [self.num_relation, self.arity, 1]\n",
    "\n",
    "        # Variable box shape\n",
    "        base_shape = rel_tbl_shape\n",
    "        # the shape is learnable, define variables accordingly\n",
    "        self.rel_shapes = Variable(torch.randn(rel_tbl_shape), requires_grad=True)\n",
    "        # TODO: nomalization\n",
    "        self.norm_rel_shapes = self.rel_shapes\n",
    "\n",
    "        self.rel_bases, self.rel_deltas = \\\n",
    "            instantiate_box_embeddings(scale_multiples_shape, rel_tbl_shape,\n",
    "                                       self.norm_rel_shapes)\n",
    "            \n",
    "    \n",
    "    def to(self, device):\n",
    "      self.entity_points = self.entity_points.to(device)\n",
    "      self.entity_bumps = self.entity_bumps.to(device)\n",
    "      self.rel_bases.requires_grad = False\n",
    "      self.rel_bases = self.rel_bases.to(device)\n",
    "      self.rel_bases.requires_grad = True\n",
    "      self.rel_deltas.requires_grad = False\n",
    "      self.rel_deltas = self.rel_deltas.to(device)\n",
    "      self.rel_deltas.requires_grad = True\n",
    "      return self\n",
    "            \n",
    "    def __call__(self, data):\n",
    "      return self.forward(data)\n",
    "\n",
    "    def forward(self, data):\n",
    "        entity_base = self.entity_points(data[:, 1: self.arity + 1].to(torch.int64).to(device)).to(device)\n",
    "        bump_emb = self.entity_bumps(data[:, 1: self.arity + 1].to(torch.int64).to(device)).to(device)\n",
    "        # Application of bumps\n",
    "        bump_sum = bump_emb.sum(dim=1, keepdims=True)\n",
    "        entity_final_emb = entity_base + bump_sum - bump_emb\n",
    "\n",
    "        # look up relation embedding\n",
    "        batch_rel_bases = torch.index_select(self.rel_bases, 0, data[:, 0].to(torch.int64)).to(device)\n",
    "        batch_rel_deltas = torch.index_select(self.rel_deltas, 0, data[:, 0].to(torch.int64)).to(device)\n",
    "        # batch_rel_mults = torch.index_select(self.rel_multiples, 0, data[:, 0])\n",
    "\n",
    "        box_low, box_high = compute_box(batch_rel_bases, batch_rel_deltas)\n",
    "        return entity_final_emb, box_low, box_high\n",
    "\n",
    "    def forward_negatives(self, negatives):\n",
    "        entities = []\n",
    "        box_low = []\n",
    "        box_high = []\n",
    "        for data in negatives:\n",
    "          e, l, h = self.forward(data)\n",
    "          entities.append(e)\n",
    "          box_low.append(l)\n",
    "          box_high.append(h)\n",
    "        return torch.stack(entities), torch.stack(box_low), torch.stack(box_high)\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.rel_bases, self.rel_deltas, self.entity_points.weight, self.entity_bumps.weight]\n",
    "\n",
    "    def parameter_dict(self):\n",
    "      return {'rel_bases': self.rel_bases,\n",
    "              'rel_deltas': self.rel_deltas,\n",
    "              # 'rel_multiples': self.rel_multiples,\n",
    "              'entity_points': self.entity_points.weight,\n",
    "              'entity_bumps': self.entity_bumps.weight}\n",
    "\n",
    "    def load_parameters(self, param_dict):\n",
    "      self.rel_bases = param_dict['rel_bases']\n",
    "      self.rel_deltas = param_dict['rel_deltas']\n",
    "      # self.rel_multiples = param_dict['rel_multiples']\n",
    "      self.entity_points.weight = param_dict['entity_points']\n",
    "      self.entity_bumps.weight = param_dict['entity_bumps']\n",
    "\n",
    "def add_padding(input_tensor): # Jiaqi\n",
    "    return torch.cat([input_tensor, torch.zeros([1, input_tensor.shape[1]])], axis=0)\n",
    "\n",
    "def instantiate_box_embeddings(scale_mult_shape, rel_tbl_shape, base_norm_shapes): # Jiaqi\n",
    "    # scale_multiples = Variable(torch.randn(scale_mult_shape), requires_grad=True)\n",
    "    # scale_multiples = F.elu(scale_multiples) + 1.0\n",
    "\n",
    "    embedding_base_points = Variable(torch.randn(rel_tbl_shape), requires_grad=True) #box base points\n",
    "    embedding_deltas = base_norm_shapes\n",
    "    return embedding_base_points, embedding_deltas\n",
    "    # embedding_deltas = torch.mul(scale_multiples, base_norm_shapes) #box width\n",
    "    # return embedding_base_points, embedding_deltas, scale_multiples\n",
    "\n",
    "def compute_box(box_base, box_delta): #Jiaqi\n",
    "    box_second = box_base + 0.5 * box_delta\n",
    "    box_first = box_base - 0.5 * box_delta\n",
    "    box_low = torch.minimum(box_first, box_second)\n",
    "    box_high = torch.maximum(box_first, box_second)\n",
    "    return box_low, box_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWUsYnb2izhG"
   },
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# data.py\n",
    "#----------------------\n",
    "\n",
    "RELATION_INDEX = 0\n",
    "HEAD_INDEX = 1\n",
    "TAIL_INDEX = 2\n",
    "\n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, opt, truncate=-1):\n",
    "        # Directory path and batch size parameters\n",
    "        self.data_path = opt.data_path\n",
    "        self.data_name = opt.data_name\n",
    "        self.batch_size = opt.batch_size\n",
    "\n",
    "        # Parse data into numpy arrays\n",
    "        self.train_data = self.parse_kg('train.txt')\n",
    "        self.valid_data = self.parse_kg('valid.txt')\n",
    "        self.test_data = self.parse_kg('test.txt')\n",
    "        if truncate > 0:  # use only first few rows; for debugging/testing\n",
    "          self.train_data = self.train_data[:truncate]\n",
    "          self.valid_data = self.valid_data[:truncate]\n",
    "          self.test_data = self.test_data[:truncate]\n",
    "        \n",
    "        # Set number of entities, relations, and set dictionaries\n",
    "        self.num_entity, self.num_relation, \\\n",
    "        self.entity_to_id_dict, self.id_to_entity_dict, \\\n",
    "        self.relation_to_id_dict, self.id_to_relation_dict = self.ret_kg_info()\n",
    "        \n",
    "        # Convert data to ids\n",
    "        self.train_data_ids = self.data_to_ids(self.train_data)\n",
    "        self.valid_data_ids = self.data_to_ids(self.valid_data)\n",
    "        self.test_data_ids = self.data_to_ids(self.test_data)\n",
    "        \n",
    "        # Create set of all facts from train, validation, and test\n",
    "        self.facts_set = self.data_to_set(np.concatenate((self.train_data, self.valid_data, self.test_data), 0))\n",
    "\n",
    "        # Create data loaders\n",
    "        self.train_loader, self.valid_loader, self.test_loader = self.get_loaders()\n",
    "\n",
    "    '''\n",
    "    Parses txt data file and returns data in numpy format\n",
    "    \n",
    "    @param train_test_or_val either Strings 'train.txt', 'test.txt', or 'valid.txt' to determine file name\n",
    "    @return data in numpy format\n",
    "    '''\n",
    "    def parse_kg(self, train_test_or_val):\n",
    "        path_to_kg = self.data_path + self.data_name + train_test_or_val\n",
    "        data = pd.read_table(path_to_kg, delim_whitespace=True, names=('head_entity', 'rel_name', 'tail_entity')).to_numpy()\n",
    "        data[:, [1, 0]] = data[:, [0, 1]] # Gets data in [relation, head, tail] format\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    '''\n",
    "    Returns number of relations, entities, and dictionaries between relations, entities, and their ids\n",
    "    \n",
    "    @param None\n",
    "    @return number of distinct entities in all datasets (train, test, and validation),\n",
    "            number of distinct relations in all datasets\n",
    "            dictionary mapping from entity names to entity ids\n",
    "            dictionary mapping from entity ids to entity names\n",
    "            dictionary mapping from relation names to relation ids\n",
    "            dictionary mapping from relation ids to relation names\n",
    "    '''\n",
    "    def ret_kg_info(self):\n",
    "        unique_heads = np.unique(np.concatenate((self.test_data[:,HEAD_INDEX], self.train_data[:,HEAD_INDEX], self.valid_data[:,HEAD_INDEX]), 0))\n",
    "        unique_tails = np.unique(np.concatenate((self.test_data[:,TAIL_INDEX], self.train_data[:,TAIL_INDEX], self.valid_data[:,TAIL_INDEX]), 0))\n",
    "        unique_ents = np.unique(np.concatenate((unique_heads, unique_tails), 0))\n",
    "\n",
    "        unique_rels = np.unique(np.concatenate((self.test_data[:,RELATION_INDEX], self.train_data[:,RELATION_INDEX], self.valid_data[:,RELATION_INDEX]), 0))\n",
    "\n",
    "        ent_to_id_dict, id_to_ent_dict = self.get_entity_dicts(unique_ents)\n",
    "        rel_to_id_dict, id_to_rel_dict = self.get_relation_dicts(unique_rels)\n",
    "\n",
    "        return [len(unique_ents), len(unique_rels), ent_to_id_dict, id_to_ent_dict, rel_to_id_dict, id_to_rel_dict]\n",
    "\n",
    "    '''\n",
    "    Creates dictionaries mapping to and from entity names (Strings) and ids\n",
    "    \n",
    "    @param the list of distinct entities\n",
    "    @return dictionary mapping from entity names to ids and dictionary mapping from entity ids to names\n",
    "    '''\n",
    "    def get_entity_dicts(self, unique_ents):\n",
    "        ent_to_id_dict = {}\n",
    "        id_to_ent_dict = {}\n",
    "        for i in range(len(unique_ents)):\n",
    "            ent_to_id_dict[unique_ents[i]] = i\n",
    "            id_to_ent_dict[i] = unique_ents[i]\n",
    "        \n",
    "        return [ent_to_id_dict, id_to_ent_dict]\n",
    "    \n",
    "    '''\n",
    "    Creates dictionaries mapping to and from relation names (Strings) and ids\n",
    "    \n",
    "    @param the list of distinct relation\n",
    "    @return dictionary mapping from relation names to ids and dictionary mapping from relation ids to names\n",
    "    '''\n",
    "    def get_relation_dicts(self, unique_rels):\n",
    "        rel_to_id_dict = {}\n",
    "        id_to_rel_dict = {}\n",
    "        for i in range(len(unique_rels)):\n",
    "            rel_to_id_dict[unique_rels[i]] = i\n",
    "            id_to_rel_dict[i] = unique_rels[i]\n",
    "            \n",
    "        return [rel_to_id_dict, id_to_rel_dict]\n",
    "    \n",
    "    '''\n",
    "    Takes data with string names (entities or relations) and returns data with ids instead\n",
    "    \n",
    "    @param the data to be mapped to ids\n",
    "    @return numpy holding ids of all data\n",
    "    '''\n",
    "    def data_to_ids(self, data):\n",
    "        ids = []\n",
    "        for i in range(len(data)):\n",
    "            arr = np.empty(3)\n",
    "            arr[RELATION_INDEX] = self.relation_to_id_dict[data[i][RELATION_INDEX]]\n",
    "            arr[HEAD_INDEX] = self.entity_to_id_dict[data[i][HEAD_INDEX]]\n",
    "            arr[TAIL_INDEX] = self.entity_to_id_dict[data[i][TAIL_INDEX]]\n",
    "            ids.append(arr)\n",
    "            \n",
    "        return np.array(ids, dtype=np.int64)\n",
    "\n",
    "    '''\n",
    "    Takes numpy array of [relation, head, tail] arrays and turns it into Python set of tuples\n",
    "    \n",
    "    @param data to be transformed\n",
    "    @return python set of (relation, head, tail) tuples\n",
    "    '''\n",
    "    def data_to_set(self, data):\n",
    "        data_list = data.tolist()\n",
    "        return set(tuple(x) for x in data_list)\n",
    "        \n",
    "    '''\n",
    "    Returns data loaders fro training, validation, and testing sets\n",
    "    \n",
    "    @param the batch size for the loades\n",
    "    @return data loader objects for train, validation, and test\n",
    "    '''\n",
    "    def get_loaders(self):\n",
    "        return [torch.utils.data.DataLoader(self.train_data_ids, self.batch_size, shuffle=True), \\\n",
    "                torch.utils.data.DataLoader(self.valid_data_ids, self.batch_size, shuffle=True), \\\n",
    "                torch.utils.data.DataLoader(self.test_data_ids, self.batch_size, shuffle=True)]\n",
    "\n",
    "    def resample(self, triples):\n",
    "      no_replacement_performed = True\n",
    "      for i, r in enumerate(triples[0]):\n",
    "        if (r, triples[1, i], triples[2, i]) in self.train_data:\n",
    "          no_replacement_performed = False\n",
    "          new_index = torch.randint(self.num_entity, (1,))\n",
    "          new_e = torch.tensor(list(self.id_to_entity_dict.keys()))[new_index].to(device)\n",
    "          is_head = torch.randint(2, (1,)) == 1\n",
    "          if is_head.item():\n",
    "            triples[1,i] = new_e.item()\n",
    "          else:\n",
    "            triples[2,i] = new_e.item()\n",
    "      return triples, no_replacement_performed\n",
    "\n",
    "    \n",
    "    def sample_negatives(self, triples, nb_samples):\n",
    "      triples_t = triples.transpose(0,1)\n",
    "      batch_size = len(triples_t[0])\n",
    "      triples_rep = torch.repeat_interleave(triples_t, nb_samples, dim=1)\n",
    "      # sample random entities\n",
    "      sample_index = torch.randint(self.num_entity, size=(batch_size*nb_samples,))\n",
    "      sample_ids = torch.tensor(list(self.id_to_entity_dict.keys()))[sample_index].to(device)\n",
    "      is_head = (torch.randint(2, size=(batch_size*nb_samples,)) == 1) # indicate if head is being replaced (otherwise, replace tail)\n",
    "      # create sampled triples from sampled entities\n",
    "      replace_mask = torch.stack((torch.zeros(len(is_head)), is_head, ~is_head)).to(device)\n",
    "      inverse_replace_mask = torch.stack((torch.ones(len(is_head)), ~is_head, is_head)).to(device)\n",
    "      replacements = torch.stack((triples_rep[0], sample_ids, sample_ids)).to(device)\n",
    "      sampled_triples = (replace_mask * replacements + inverse_replace_mask * triples_rep).to(device)\n",
    "      # filter out and replace known positive triples\n",
    "      filtering_done = False\n",
    "      while not filtering_done:\n",
    "        sampled_triples, filtering_done = self.resample(sampled_triples)\n",
    "      return sampled_triples.reshape((3, batch_size, nb_samples)).long().transpose(0,2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BoxE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
